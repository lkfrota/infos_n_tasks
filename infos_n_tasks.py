# Fluxo global do aplicaitvo

# Por um bot do Telegram o usuÃ¡rio envia uma mensagem especificando o que quer fazer
# Exemplo: "Oi, tenho um novo item para minha caixa de entrada." ou "Coloca isso na minha caixa de entrada"
# E o aplicativo, suportado nessa etapa por um llm, entende e aguarda o tal item.

# Seguindo esse fluxo o usuÃ¡rio escreve o tal conteÃºdo para a caixa de entrada
# O llm capta o tal conteÃºdo e grava na base de dados conforme a classe CaixaEntrada definida em modelo.py

# Em seguida o llm informa o usuÃ¡rio do estado da caixa de entrada: quantos itens, Ãºltima vez que processou os itens de lÃ¡, etc.
# O llm tambÃ©m oferece de processar os itens agora

# O usuÃ¡rio envia uma mensagem pelo bot Telegram informando que quer processar a caixa de entrada agora
# O llm inicia o loop de processamento da Caixa de entrada conforme existente no graph.py ou graph_langgraph_backup.py
# O processamento acontece pelo chat do bot Telegram


import os
from dotenv import load_dotenv
from typing import Optional
from modelo import session, CaixaEntrada
from graph import processar_item_com_llm, salvar_proposta, remover_item_da_caixa_entrada
from pydantic import BaseModel, Field
from functools import wraps

# Telegram bot (python-telegram-bot v21+)
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, ContextTypes, filters

# LLM setup
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage
from langchain_core.tools import tool

load_dotenv(override=True)
allowed_user = int(os.getenv("TELEGRAM_ALLOWED_USER"))
bot_token = os.getenv("TELEGRAM_BOT_TOKEN")

# Initialize LLM
llm = init_chat_model("google_genai:gemini-2.0-flash-lite")

# System prompt for the Telegram bot
SYSTEM_PROMPT = """VocÃª Ã© um assistente pessoal para gestÃ£o de informaÃ§Ãµes, ideias e tarefas.

VocÃª pode ajudar o usuÃ¡rio a:
- Adicionar itens Ã  Caixa de Entrada
- Verificar o status da Caixa de Entrada
- Processar itens da Caixa de Entrada

Quando o usuÃ¡rio quiser adicionar algo Ã  Caixa de Entrada, use a ferramenta disponÃ­vel.
Seja amigÃ¡vel e Ãºtil nas suas respostas."""

class EstadoProcessamento:
    def __init__(self):
        self.processando = False
        self.aguardando_revisao = False
        self.proposta_atual = None
        self.item_atual = None
        self.messages_history = None

# InstÃ¢ncia global
estado_processamento = EstadoProcessamento()

def restricted(func):
    @wraps(func)
    async def wrapped(update: Update, context: ContextTypes.DEFAULT_TYPE, *args, **kwargs):
        user_id = update.effective_user.id
        
        if user_id != allowed_user:
            await update.message.reply_text(f"Desculpe, nÃ£o tenho permissÃ£o para falar com vocÃª: {user_id}")
            print(f"Acesso negado para o User ID: {user_id}")
            return
        
        return await func(update, context, *args, **kwargs)
    
    return wrapped

# Tool definition for adding to Caixa de Entrada
@tool
def adicionar_na_caixa_entrada(conteudo: str) -> str:
    """Adiciona um item Ã  Caixa de Entrada do usuÃ¡rio.
    
    Args:
        conteudo: O texto a ser adicionado Ã  Caixa de Entrada
        
    Returns:
        ConfirmaÃ§Ã£o da adiÃ§Ã£o com o ID do item
    """
    item = CaixaEntrada(conteudo_bruto=conteudo.strip())
    session.add(item)
    session.commit()
    return f"Item adicionado Ã  Caixa de Entrada com ID {item.id}"

@tool
def verificar_status_caixa_entrada() -> str:
    """Verifica quantos itens hÃ¡ na Caixa de Entrada.
    
    Returns:
        NÃºmero de itens na Caixa de Entrada
    """
    total = session.query(CaixaEntrada).count()
    return f"HÃ¡ {total} itens na Caixa de Entrada"

@tool
def processar_caixa_entrada() -> str:
    """Processa todos os itens da Caixa de Entrada.
    
    Returns:
        ConfirmaÃ§Ã£o do processamento
    """
    return "Iniciando processamento da Caixa de Entrada via Telegram..."

# Bind tools to LLM
llm_with_tools = llm.bind_tools([adicionar_na_caixa_entrada, verificar_status_caixa_entrada, processar_caixa_entrada])
# Global conversation history (single user)
conversation_history = [SystemMessage(content=SYSTEM_PROMPT)]

@restricted
async def process_message_with_llm(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Process user message with LLM and handle tool calls."""
    
    # Se estÃ¡ aguardando revisÃ£o, processar resposta
    if estado_processamento.aguardando_revisao:
        await processar_resposta_revisao(update, context)
        return
    
    if update.message is None or update.message.text is None:
        return
    
    user_message = update.message.text.strip()
    
    if not user_message:
        return
    
    # Add user message to conversation
    conversation_history.append(HumanMessage(content=user_message))
    
    try:
        # Get LLM response
        response = llm_with_tools.invoke(conversation_history)
        
        # Add AI response to conversation
        conversation_history.append(response)
        
        # Check if LLM wants to call tools
        if response.tool_calls:
            # Execute tool calls
            for tool_call in response.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]
                
                if tool_name == "adicionar_na_caixa_entrada":
                    result = adicionar_na_caixa_entrada.invoke(tool_args)
                elif tool_name == "verificar_status_caixa_entrada":
                    result = verificar_status_caixa_entrada.invoke(tool_args)
                elif tool_name == "processar_caixa_entrada":
                    result = await processar_caixa_entrada_telegram(update, context)
                else:
                    result = f"Ferramenta {tool_name} nÃ£o reconhecida"
                
                # Add tool result to conversation
                conversation_history.append(ToolMessage(content=result, tool_call_id=tool_call["id"]))
            
            # Get final response after tool execution
            final_response = llm_with_tools.invoke(conversation_history)
            conversation_history.append(final_response)
            
            await update.message.reply_text(final_response.content)
        else:
            # No tool calls, just respond
            await update.message.reply_text(response.content)
            
    except Exception as e:
        await update.message.reply_text(f"Erro ao processar mensagem: {str(e)}")

@restricted
async def processar_resposta_revisao(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Processa resposta do usuÃ¡rio Ã  revisÃ£o."""
    resposta = update.message.text.strip().lower()
    
    if resposta in ['s', 'sim', 'y', 'yes', 'ok']:
        # Aprovado - salvar e continuar
        from graph import salvar_proposta, remover_item_da_caixa_entrada
        if salvar_proposta(estado_processamento.proposta_atual):
            if remover_item_da_caixa_entrada(estado_processamento.item_atual.id):
                await update.message.reply_text("âœ… Item aprovado e salvo! Continuando processamento...")
                # Continuar processamento
                await processar_caixa_entrada_telegram(update, context)
    elif resposta in ['n', 'nÃ£o', 'nao', 'no']:
        # Rejeitado
        await update.message.reply_text("âŒ Item rejeitado, mantido na Caixa de Entrada.")
        estado_processamento.aguardando_revisao = False
    else:
        # Feedback - reprocessar
        await update.message.reply_text("ðŸ“ Feedback recebido, reprocessando...")
        # Adicionar feedback ao histÃ³rico e reprocessar
        estado_processamento.messages_history.append(AIMessage(content=f"SugestÃ£o anterior: {estado_processamento.proposta_atual.model_dump_json()}"))
        estado_processamento.messages_history.append(HumanMessage(content=f"Feedback do usuÃ¡rio: {resposta}"))
        await processar_caixa_entrada_telegram(update, context)

@restricted
async def processar_caixa_entrada_telegram(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Processa Caixa de Entrada via Telegram."""
    from graph import processar_item_com_llm, salvar_proposta, remover_item_da_caixa_entrada
    from modelo import session, CaixaEntrada
    from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
    from prompts import PROMPT_ORGANIZADOR
    
    print("=== Iniciando processamento da Caixa de Entrada ===")
    
    while True:
        # 1. Buscar prÃ³ximo item
        item = session.query(CaixaEntrada).order_by(CaixaEntrada.id.asc()).first()
        if not item:
            print("âœ… Nenhum item na Caixa de Entrada. Processamento encerrado.")
            await update.message.reply_text("âœ… Processamento concluÃ­do! Nenhum item restante na Caixa de Entrada.")
            return "Processamento concluÃ­do!"
            
        print(f"\nï¿½ï¿½ Processando Item {item.id}")
        print(f"ConteÃºdo: {item.conteudo_bruto[:100]}...")
        
        # 2. Processar com LLM
        messages_history = [
            SystemMessage(content=PROMPT_ORGANIZADOR),
            HumanMessage(content=f"\nTexto para anÃ¡lise:\n{item.conteudo_bruto}")
        ]
        
        # 3. Loop de processamento com feedback
        while True:
            try:
                proposal = processar_item_com_llm(item.conteudo_bruto, messages_history)
            except Exception as e:
                print(f"âŒ Erro ao processar item com LLM: {e}")
                break
            
            # 4. Review gate - AGORA VIA TELEGRAM
            if not proposal.aprovado:
                # Configurar estado para aguardar revisÃ£o
                estado_processamento.aguardando_revisao = True
                estado_processamento.proposta_atual = proposal
                estado_processamento.item_atual = item
                estado_processamento.messages_history = messages_history
                
                # Enviar proposta via Telegram
                await update.message.reply_text(
                    f"ðŸ“‹ **PROPOSTA PARA REVISÃƒO**\n\n"
                    f"**InformaÃ§Ãµes:** {proposal.informacoes}\n"
                    f"**Ideias:** {proposal.ideias}\n"
                    f"**Tarefas:** {proposal.tarefas}\n\n"
                    f"Sua aprovaÃ§Ã£o: ('s', 'n' ou feedback)"
                )
                
                # SAIR do loop e aguardar resposta do usuÃ¡rio
                return "Aguardando revisÃ£o e resposta do usuÃ¡rio."
            
            else:
                # JÃ¡ aprovado pelo LLM
                break
        
        # 5. Salvar e remover item
        if salvar_proposta(proposal):
            if remover_item_da_caixa_entrada(item.id):
                print(f"âœ… Item {item.id} processado com sucesso!")
                await update.message.reply_text(f"âœ… Item {item.id} processado e salvo!")
    
    return "Processamento concluÃ­do!"

@restricted
async def cmd_help(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "Comandos disponÃ­veis:\n"
        "/help - Ver esta ajuda\n\n"
        "Ou simplesmente converse comigo em linguagem natural!\n"
        "Exemplos:\n"
        "â€¢ 'Adicione Ã  caixa de entrada: preciso comprar leite'\n"
        "â€¢ 'Quantos itens tenho pendentes?'\n"
        "â€¢ 'Processe minha caixa de entrada'\n"
        "â€¢ 'Coloque na minha lista que vou viajar em dezembro'"
    )

def main(token: Optional[str] = None) -> None:
    
    app = Application.builder().token(bot_token).build()

    app.add_handler(CommandHandler("help", cmd_help))
    
    # Process all text messages with LLM
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, process_message_with_llm))

    print("Bot iniciado com LLM. Converse naturalmente no Telegram!")
    app.run_polling(close_loop=False)


if __name__ == "__main__":
    main()
